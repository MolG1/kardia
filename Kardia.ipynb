{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Import and install libraries, the Diorisis corpus and List's stopwords"
      ],
      "metadata": {
        "id": "GAB-LUFvUYLA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e9RwjT1og4U"
      },
      "outputs": [],
      "source": [
        "!pip install cltk\n",
        "from cltk.alphabet.grc.beta_to_unicode import BetaCodeReplacer\n",
        "from cltk.alphabet.grc.grc import tonos_oxia_converter\n",
        "from glob import glob\n",
        "from xml.etree.ElementTree import parse\n",
        "import re\n",
        "import os\n",
        "import os.path\n",
        "import pandas as pd\n",
        "\n",
        "# Fetch List's stopwords and the Diorisis Corpus\n",
        "\n",
        "!git clone https://github.com/lisni946/ijl_greek_kinship_terms\n",
        "!wget -O Diorisis.zip \"https://figshare.com/ndownloader/files/11296247\"\n",
        "!unzip Diorisis.zip -d /content/corpus\n",
        "new_stops = os.path.join(\"/content/ijl_greek_kinship_terms/new_stops.csv\")\n",
        "f = open(new_stops)\n",
        "X = pd.read_csv(f, delimiter=\",\", )\n",
        "X.head()\n",
        "df = pd.DataFrame(X, columns=['Add Stops'])\n",
        "new_list = df['Add Stops'].values.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extract and save metadata from the Diorisis corpus (genre, subgenre, year)"
      ],
      "metadata": {
        "id": "rhvYdvyLUnhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import glob\n",
        "from xml.etree.ElementTree import parse\n",
        "\n",
        "def save_metadata(metadata, filename):\n",
        "    with open(filename, \"wb\") as f:\n",
        "        pickle.dump(metadata, f)\n",
        "\n",
        "xml_files = glob.glob('/content/corpus/*.xml')\n",
        "corpus = []\n",
        "metadata = []\n",
        "for xml in xml_files:\n",
        "    with open(xml, 'r') as x:\n",
        "        tree = parse(x)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        genre = root.find('.//xenoData/genre').text\n",
        "        subgenre = root.find('.//xenoData/subgenre').text\n",
        "        year = int(root.find('.//profileDesc/creation/date').text)\n",
        "\n",
        "        metadata.append((genre, subgenre, year))\n",
        "\n",
        "# Save metadata\n",
        "metadata_file = \"metadata.pkl\"\n",
        "save_metadata(metadata, metadata_file)"
      ],
      "metadata": {
        "id": "MuYzisywoiBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set searchword, define corpus A and Corpus B and set the number of models to be trained"
      ],
      "metadata": {
        "id": "COTLI5tSU9Ak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_word = 'καρδία' # Set search_word (for all corpuses)\n",
        "\n",
        "# Define corpus A\n",
        "desired_genre_A = \"Religion\"\n",
        "desired_subgenre_A = None\n",
        "desired_year_start_A = -800\n",
        "desired_year_end_A = 365\n",
        "\n",
        "# Define corpus B\n",
        "desired_genre_B = \"Technical\"\n",
        "desired_subgenre_B = \"Medicine\"\n",
        "desired_year_start_B = -800\n",
        "desired_year_end_B = 170\n",
        "\n",
        "# Set the number of models\n",
        "num_models_A = 30 #Corpus A\n",
        "num_models_B = 30 #Corpus B"
      ],
      "metadata": {
        "id": "DD0aye1QolNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train models on corpus A"
      ],
      "metadata": {
        "id": "kbMTwDhCVFMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "import os\n",
        "import glob\n",
        "import multiprocessing\n",
        "import gensim.models.word2vec as w2v\n",
        "import sklearn.manifold\n",
        "import time\n",
        "from xml.etree.ElementTree import parse\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "from typing import Optional, List, Tuple\n",
        "\n",
        "# Generate randomized parameters\n",
        "def generate_random_params():\n",
        "    num_features = random.randint(100, 300)\n",
        "    context_size = random.randint(5, 10)\n",
        "    return num_features, context_size\n",
        "\n",
        "# Load metadata\n",
        "with open('metadata.pkl', 'rb') as f:\n",
        "    metadata = pickle.load(f)\n",
        "\n",
        "xml_files = glob.glob('/content/corpus/*.xml')\n",
        "\n",
        "# Define the BetaCodeReplacer class\n",
        "class BetaCodeReplacer:\n",
        "    def __init__(self, pattern: Optional[List[Tuple[str, str]]] = None, reorder_pattern: Optional[List[Tuple[str, str]]] = None):\n",
        "        pass\n",
        "\n",
        "beta_code_replace = BetaCodeReplacer()\n",
        "\n",
        "corpus_A = []\n",
        "\n",
        "for xml, (genre, subgenre, year) in zip(xml_files, metadata):\n",
        "   if (desired_genre_A is None or genre == desired_genre_A) and (desired_subgenre_A is None or subgenre == desired_subgenre_A) and desired_year_start_A <= year <= desired_year_end_A:\n",
        "        with open(xml, 'r') as x:\n",
        "            tree = parse(x)\n",
        "            root = tree.getroot()\n",
        "            for sentence in root.iter('sentence'):\n",
        "                sentences = []\n",
        "                for word in sentence.iter('word'):\n",
        "                    for lemma in word.iter('lemma'):\n",
        "                        entry = lemma.get('entry')\n",
        "                        if entry is None:\n",
        "                            entry = word.get('form')\n",
        "                            sentences.append(entry)\n",
        "                        elif tonos_oxia_converter(entry) not in new_list:\n",
        "                            sentences.append(entry)\n",
        "                if len(sentences) > 0:\n",
        "                    corpus_A.append(sentences)\n",
        "\n",
        "# Train models\n",
        "models_A = []\n",
        "\n",
        "for i in range(num_models_A):\n",
        "    num_features, context_size = generate_random_params()\n",
        "\n",
        "    seed = i\n",
        "    downsampling = 1e-3\n",
        "    num_workers = multiprocessing.cpu_count()\n",
        "    min_word_count = 10\n",
        "\n",
        "    greek2vec = w2v.Word2Vec(\n",
        "        sg=1,\n",
        "        seed=seed,\n",
        "        workers=num_workers,\n",
        "        vector_size=num_features,\n",
        "        min_count=min_word_count,\n",
        "        window=context_size,\n",
        "        sample=downsampling\n",
        "    )\n",
        "    greek2vec.build_vocab(corpus_A)\n",
        "\n",
        "    token_count_A = sum([len(sentence) for sentence in corpus_A])\n",
        "    print('Model {}/{} - Num Features: {}, Context Size: {}, Tokens: {:,}'.format(i+1, num_models_A, num_features, context_size, token_count_A))\n",
        "\n",
        "    greek2vec.train(corpus_A, total_examples=greek2vec.corpus_count, epochs=50)\n",
        "    models_A.append(greek2vec)\n",
        "\n",
        "    greek2vec.save(f\"{search_word}_{desired_genre_A}_{desired_subgenre_A}_{desired_year_start_A}_to_{desired_year_end_A}_model_{i+1}.model\")"
      ],
      "metadata": {
        "id": "mklFt_jBomkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train models on corpus B"
      ],
      "metadata": {
        "id": "MhswGWCWVMHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "import os\n",
        "import glob\n",
        "import multiprocessing\n",
        "import gensim.models.word2vec as w2v\n",
        "import sklearn.manifold\n",
        "import time\n",
        "from xml.etree.ElementTree import parse\n",
        "from collections import Counter\n",
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "from typing import Optional, List, Tuple\n",
        "\n",
        "# Generate randomized parameters\n",
        "def generate_random_params():\n",
        "    num_features = random.randint(100, 300)\n",
        "    context_size = random.randint(5, 10)\n",
        "    return num_features, context_size\n",
        "\n",
        "# Load metadata\n",
        "with open('metadata.pkl', 'rb') as f:\n",
        "    metadata = pickle.load(f)\n",
        "\n",
        "xml_files = glob.glob('/content/corpus/*.xml')\n",
        "\n",
        "# Define the BetaCodeReplacer class\n",
        "class BetaCodeReplacer:\n",
        "    def __init__(self, pattern: Optional[List[Tuple[str, str]]] = None, reorder_pattern: Optional[List[Tuple[str, str]]] = None):\n",
        "        pass\n",
        "\n",
        "beta_code_replace = BetaCodeReplacer()\n",
        "\n",
        "corpus_B = []\n",
        "\n",
        "for xml, (genre, subgenre, year) in zip(xml_files, metadata):\n",
        "    if (desired_genre_B is None or genre == desired_genre_B) and (desired_subgenre_B is None or subgenre == desired_subgenre_B) and desired_year_start_B <= year <= desired_year_end_B:\n",
        "        with open(xml, 'r') as x:\n",
        "            tree = parse(x)\n",
        "            root = tree.getroot()\n",
        "            for sentence in root.iter('sentence'):\n",
        "                sentences = []\n",
        "                for word in sentence.iter('word'):\n",
        "                   for lemma in word.iter('lemma'):\n",
        "                        entry = lemma.get('entry')\n",
        "                        if entry is None:\n",
        "                            entry = word.get('form')\n",
        "                            sentences.append(entry)\n",
        "                        elif tonos_oxia_converter(entry) not in new_list:\n",
        "                            sentences.append(entry)\n",
        "                if len(sentences) > 0:\n",
        "                    corpus_B.append(sentences)\n",
        "\n",
        "# Train models\n",
        "models_B = []\n",
        "\n",
        "for i in range(num_models_B):\n",
        "    num_features, context_size = generate_random_params()\n",
        "\n",
        "    seed = i\n",
        "    downsampling = 1e-3\n",
        "    num_workers = multiprocessing.cpu_count()\n",
        "    min_word_count = 10\n",
        "\n",
        "    greek2vec = w2v.Word2Vec(\n",
        "        sg=1,\n",
        "        seed=seed,\n",
        "        workers=num_workers,\n",
        "        vector_size=num_features,\n",
        "        min_count=min_word_count,\n",
        "        window=context_size,\n",
        "        sample=downsampling\n",
        "    )\n",
        "    greek2vec.build_vocab(corpus_B)\n",
        "\n",
        "    token_count_B = sum([len(sentence) for sentence in corpus_B])\n",
        "    print('Model {}/{} - Num Features: {}, Context Size: {}, Tokens: {:,}'.format(i+1, num_models_B, num_features, context_size, token_count_B))\n",
        "\n",
        "    greek2vec.train(corpus_B, total_examples=greek2vec.corpus_count, epochs=50)\n",
        "    models_B.append(greek2vec)\n",
        "\n",
        "    greek2vec.save(f\"{search_word}_{desired_genre_B}_{desired_subgenre_B}_{desired_year_start_B}_to_{desired_year_end_B}_model_{i+1}.model\")"
      ],
      "metadata": {
        "id": "eGEbHiubopv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Print results for models trained on corpus A"
      ],
      "metadata": {
        "id": "vYMfaBXoVSMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Results for '{search_word}' in genre '{desired_genre_A}', subgenre '{desired_subgenre_A}', ({desired_year_start_A} to {desired_year_end_A})\")\n",
        "print(f\"Tokens: {token_count_A}\")\n",
        "\n",
        "similar_words_lists_A = []\n",
        "\n",
        "# Load each model and find most similar words to the search word\n",
        "for model in models_A:\n",
        "    similar_words_A = model.wv.most_similar(search_word, topn=100)\n",
        "    similar_words_lists_A.append(similar_words_A)\n",
        "\n",
        "# Define all_words based on similar_words_lists\n",
        "all_words_A = set(word for similar_words_A in similar_words_lists_A for word, _ in similar_words_A)\n",
        "\n",
        "# Calculate frequencies of all relevant words in the corpus\n",
        "word_freq = Counter()\n",
        "for sentence in corpus_A:\n",
        "    for word in sentence:\n",
        "        if word in all_words_A or word == search_word:\n",
        "            word_freq[word] += 1\n",
        "\n",
        "similarities_A = {}\n",
        "for word in all_words_A:\n",
        "    scores = [score for similar_words_A in similar_words_lists_A for w, score in similar_words_A if w == word]\n",
        "    if scores:  # If there are scores, i.e. the word was found in some model\n",
        "        mean_score = np.mean(scores)\n",
        "        coverage = len(scores) / len(similar_words_lists_A)  # Calculate coverage as a proportion of models\n",
        "        freq = word_freq[word]  # Get frequency from the word_freq Counter\n",
        "        similarities_A[word] = (mean_score, coverage, freq)\n",
        "\n",
        "# Sorting by mean_score primarily\n",
        "sorted_words_A = sorted(similarities_A.items(), key=lambda x: x[1][0], reverse=True)\n",
        "\n",
        "# Print the frequency of the search_word\n",
        "print(f\"\\nFrequency of '{search_word}' in the corpus: {word_freq[search_word]}\")\n",
        "\n",
        "# Print the results; the average cosine similarity score as calculated across models, the models' coverage of the word (how often it appears in each model), the frequency of the word in the defined corpus\n",
        "print(\"Top words with the highest average cosine similarity scores, their coverage, and frequency in the corpus:\")\n",
        "for word, (mean_score, coverage, freq) in sorted_words_A[:40]:\n",
        "    print(f\"{word}: Average score: {mean_score:.3f}, Coverage: {coverage:.2%}, Frequency: {freq}\")"
      ],
      "metadata": {
        "id": "KvmZ6IuJQ46A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Print results for models trained on corpus B"
      ],
      "metadata": {
        "id": "S-bhsZMsVcGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Results for '{search_word}' in genre '{desired_genre_B}', subgenre '{desired_subgenre_B}', ({desired_year_start_B} to {desired_year_end_B})\")\n",
        "\n",
        "print(f\"Tokens: {token_count_B}\")\n",
        "\n",
        "similar_words_lists_B = []\n",
        "\n",
        "# Load each model and find most similar words to the search word\n",
        "for model in models_B:\n",
        "    similar_words_B = model.wv.most_similar(search_word, topn=100)\n",
        "    similar_words_lists_B.append(similar_words_B)\n",
        "\n",
        "# Define all_words based on similar_words_lists\n",
        "all_words_B = set(word for similar_words_B in similar_words_lists_B for word, _ in similar_words_B)\n",
        "\n",
        "# Calculate frequencies of all relevant words in the corpus\n",
        "word_freq = Counter()\n",
        "for sentence in corpus_B:\n",
        "    for word in sentence:\n",
        "        if word in all_words_B or word == search_word:\n",
        "            word_freq[word] += 1\n",
        "\n",
        "similarities = {}\n",
        "for word in all_words_B:\n",
        "    scores = [score for similar_words in similar_words_lists_B for w, score in similar_words if w == word]\n",
        "    if scores:  # If there are scores, i.e. the word was found in some models\n",
        "        mean_score = np.mean(scores)\n",
        "        coverage = len(scores) / len(similar_words_lists_B)  # Calculate coverage as a proportion of models\n",
        "        freq = word_freq[word]  # Get frequency from the word_freq Counter\n",
        "        similarities[word] = (mean_score, coverage, freq)\n",
        "\n",
        "# Sorting by mean_score primarily\n",
        "sorted_words_B = sorted(similarities.items(), key=lambda x: x[1][0], reverse=True)\n",
        "\n",
        "# Print the frequency of the search_word separately\n",
        "print(f\"\\nFrequency of '{search_word}' in the corpus: {word_freq[search_word]}\")\n",
        "\n",
        "# Print the results; the average cosine similarity score as calculated across models, the models' coverage of the word (how often it appears in each model), the frequency of the word in the defined corpus\n",
        "print(\"Top words with the highest average cosine similarity scores, their coverage, and frequency in the corpus:\")\n",
        "for word, (mean_score, coverage, freq) in sorted_words_B[:40]:\n",
        "    print(f\"{word}: Average score: {mean_score:.3f}, Coverage: {coverage:.2%}, Frequency: {freq}\")"
      ],
      "metadata": {
        "id": "c1icVOr9Q5sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compare corpus A with corpus B"
      ],
      "metadata": {
        "id": "nR9Uv1LCVgnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_A_set = {word for word, data in sorted_words_A}\n",
        "words_B_set = {word for word, data in sorted_words_B}\n",
        "\n",
        "# Common words\n",
        "common_words = words_A_set.intersection(words_B_set)\n",
        "print(\"Common words:\", common_words)\n",
        "\n",
        "for word in common_words:\n",
        "    data_A = next((data for w, data in sorted_words_A if w == word), None)\n",
        "    data_B = next((data for w, data in sorted_words_B if w == word), None)\n",
        "    if data_A and data_B:\n",
        "        print(f\"Word: {word}\")\n",
        "        print(f\"\\tCorpus A - Average score: {data_A[0]:.3f}, Coverage: {data_A[1]:.2%}, Frequency: {data_A[2]}\")\n",
        "        print(f\"\\tCorpus B - Average score: {data_B[0]:.3f}, Coverage: {data_B[1]:.2%}, Frequency: {data_B[2]}\")"
      ],
      "metadata": {
        "id": "2PiXlZOrouLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download the models trained on corpus A"
      ],
      "metadata": {
        "id": "xx9RIvQQWhmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "\n",
        "# Directory where your models are saved\n",
        "directory = \"/content\"\n",
        "\n",
        "# Pattern to match filenames\n",
        "pattern_A = re.compile(rf\"{search_word}_{desired_genre_A}_{desired_subgenre_A}_{desired_year_start_A}_to_{desired_year_end_A}_model_\\d+\\.model\")\n",
        "\n",
        "# List all model files in the directory\n",
        "model_files_A = [f for f in os.listdir(directory) if pattern_A.match(f)]\n",
        "\n",
        "# Zip all model files\n",
        "zip_file_path_A = \"/content/modelscorpusA.zip\"\n",
        "with zipfile.ZipFile(zip_file_path_A, 'w') as zipf:\n",
        "    for model_file in model_files_A:\n",
        "        zipf.write(os.path.join(directory, model_file), model_file)\n",
        "\n",
        "# Download the zip file\n",
        "files.download(zip_file_path_A)"
      ],
      "metadata": {
        "id": "VKWxDvXleBVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download the models trained on corpus B"
      ],
      "metadata": {
        "id": "57GzOXqHWsFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "\n",
        "# Directory where your models are saved\n",
        "directory = \"/content\"\n",
        "\n",
        "# Pattern to match filenames\n",
        "pattern_B = re.compile(rf\"{search_word}_{desired_genre_B}_{desired_subgenre_B}_{desired_year_start_B}_to_{desired_year_end_B}_model_\\d+\\.model\")\n",
        "\n",
        "# List all model files in the directory\n",
        "model_files_B = [f for f in os.listdir(directory) if pattern_B.match(f)]\n",
        "\n",
        "# Zip all model files\n",
        "zip_file_path_B = \"/content/modelscorpusB.zip\"\n",
        "with zipfile.ZipFile(zip_file_path_B, 'w') as zipf:\n",
        "    for model_file in model_files_B:\n",
        "        zipf.write(os.path.join(directory, model_file), model_file)\n",
        "\n",
        "# Download the zip file\n",
        "files.download(zip_file_path_B)"
      ],
      "metadata": {
        "id": "Ae1M0bYzeCZi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}